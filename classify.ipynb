{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b83fbd-3edb-43d6-b088-df5bba9c3c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install neccessary libraries\n",
    "! pip install tensorflow numpy pandas transformers scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16eca189-cb06-4756-9575-6fbaebb3a16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 15:19:52.381160: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/sagemaker-user/.conda/envs/classify/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# library imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, optimizers, losses\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8425f5a6-757f-42af-a541-c6da1652aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the required file path\n",
    "df = pd.read_csv('/home/sagemaker-user/classification_model/classification_dataset.csv')\n",
    "\n",
    "# Define a function to update the category labels\n",
    "def update(cat):\n",
    "    \"\"\"\n",
    "    Update category labels.\n",
    "    \n",
    "    Args:\n",
    "    - cat (str): Category label to be updated.\n",
    "    \n",
    "    Returns:\n",
    "    - int: Updated category label (0 for \"Biology\", 1 for \"Finance\", unchanged otherwise).\n",
    "    \"\"\"\n",
    "    if cat == \"Biology\":\n",
    "        return 0\n",
    "    elif cat == \"Finance\":\n",
    "        return 1\n",
    "    return cat\n",
    "\n",
    "# Apply the update function to the 'Course' column in the DataFrame\n",
    "df.loc[:, \"Course\"] = df[\"Course\"].apply(update)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# Assign the 'Question' column to variable x and the updated 'Course' column to variable y\n",
    "x = df['Question']\n",
    "y = df['Course'].astype(np.int64)  # Update function is returning int, but TensorFlow v2 dataset accepts only int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab229e9c-353f-4ce0-98f5-33a252988e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/classify/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the model name and maximum sequence length\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LEN = 20\n",
    "\n",
    "# x is a Pandas DataFrame containing Questions, converting it to a list\n",
    "review = x.to_list()\n",
    "\n",
    "# Initializing a DistilBERT tokenizer with the specified model name\n",
    "tkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenizing the Questions using the tokenizer, ensuring all inputs have the same length\n",
    "# max_length specifies the maximum length of each input sequence after tokenization\n",
    "# truncation=True ensures that sequences longer than max_length are truncated\n",
    "# padding=True ensures that sequences shorter than max_length are padded with the appropriate token\n",
    "inputs = tkzr(review, max_length=MAX_LEN, truncation=True, padding=True)\n",
    "\n",
    "# print(f'review: \\'{review}\\'')\n",
    "# print(f'input ids: {inputs[\"input_ids\"]}')\n",
    "# print(f'attention mask: {inputs[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f58c4f-7963-4cce-9331-bd378ae846f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):\n",
    "    \"\"\"\n",
    "    Construct token encodings for the input data using the provided tokenizer.\n",
    "\n",
    "    Args:\n",
    "    - x (list): List of input data (e.g., reviews).\n",
    "    - tkzr (PreTrainedTokenizer): Tokenizer object to tokenize the input data.\n",
    "    - max_len (int): Maximum length of each tokenized sequence.\n",
    "    - truncation (bool): Whether to truncate sequences longer than max_len (default=True).\n",
    "    - padding (bool): Whether to pad sequences shorter than max_len (default=True).\n",
    "\n",
    "    Returns:\n",
    "    - encodings (dict): Token encodings generated by the tokenizer.\n",
    "    \"\"\"\n",
    "    # Tokenize the input data using the provided tokenizer and specified parameters\n",
    "    encodings = tkzr(x, max_length=max_len, truncation=truncation, padding=padding)\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "encodings = construct_encodings(x.to_list(), tkzr, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd2fda9-8118-4d5a-bb39-ba9ef76d0e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 15:20:07.865033: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "def construct_tfdataset(encodings, y=None):\n",
    "    \"\"\"\n",
    "    Construct a TensorFlow dataset from token encodings and labels (if provided).\n",
    "\n",
    "    Args:\n",
    "    - encodings (dict): Token encodings generated by a tokenizer.\n",
    "    - y (array-like, optional): Labels associated with the token encodings.\n",
    "\n",
    "    Returns:\n",
    "    - tfdataset (tf.data.Dataset): TensorFlow dataset containing token encodings and labels (if provided).\n",
    "    \"\"\"\n",
    "    if y is not None:\n",
    "        # If labels are provided, create a TensorFlow dataset with token encodings and labels\n",
    "        return tf.data.Dataset.from_tensor_slices((dict(encodings), y))\n",
    "    else:\n",
    "        # If labels are not provided (e.g., for inference/predictions), create a TensorFlow dataset with only token encodings\n",
    "        return tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "\n",
    "tfdataset = construct_tfdataset(encodings, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86a2a0b-66cb-40ad-bf2b-67f060469207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test split ratio and batch size\n",
    "TEST_SPLIT = 0.2\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "# Calculate the size of the training set based on the test split ratio\n",
    "train_size = int(len(x) * (1 - TEST_SPLIT))\n",
    "\n",
    "# Shuffle the dataset\n",
    "tfdataset = tfdataset.shuffle(len(x))\n",
    "\n",
    "# Split the shuffled dataset into training and testing sets\n",
    "tfdataset_train = tfdataset.take(train_size)\n",
    "tfdataset_test = tfdataset.skip(train_size)\n",
    "\n",
    "# Batch the training and testing datasets\n",
    "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\n",
    "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d27d79f-3bd7-417f-acdf-de9dd9b6ed78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 15:20:14.713634: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32 and shape [40,20]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-05-29 15:20:14.713980: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [40,20]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 22s 487ms/step - loss: 0.6459 - accuracy: 0.7500\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 8s 476ms/step - loss: 0.4208 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 8s 485ms/step - loss: 0.1806 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 8s 482ms/step - loss: 0.0788 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 8s 488ms/step - loss: 0.0447 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 7s 466ms/step - loss: 0.0279 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 8s 472ms/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 8s 483ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 8s 487ms/step - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 8s 494ms/step - loss: 0.0118 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89cc2591d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the number of epochs\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# Initialize the DistilBERT model for sequence classification\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optimizers.Adam(learning_rate=2e-5)\n",
    "loss_fn = losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile the model with the optimizer, loss function, and metrics\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e78959-328e-432d-bf31-1624193e7efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 15:21:58.299328: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [40]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2024-05-29 15:21:58.302176: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [40,20]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 2s 50ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "{'loss': 0.008232252672314644, 'accuracy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b57a163a-9ae7-496f-8e22-967600913370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 15:22:00.060442: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [40]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2024-05-29 15:22:00.068646: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype int64 and shape [40]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 143ms/step\n",
      "1/1 [==============================] - 0s 136ms/step\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "y_test: [0 0 1 0 1 0 0 1]\n",
      "y_pred: [0 1 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the true labels from the test dataset\n",
    "y_test = []\n",
    "\n",
    "# Iterate through the test dataset to extract true labels\n",
    "for _, label in tfdataset_test.unbatch():\n",
    "    y_test.append(label.numpy())\n",
    "\n",
    "# Convert the list of true labels to a NumPy array\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Initialize an empty list to store the predicted labels\n",
    "y_pred = []\n",
    "\n",
    "# Iterate through batches of the test dataset to predict labels\n",
    "for batch in tfdataset_test:\n",
    "    # Extract inputs from the batch (excluding labels)\n",
    "    inputs, _ = batch\n",
    "    # Use the model to predict labels for the inputs\n",
    "    preds = model.predict(inputs).logits\n",
    "    # Extract the predicted labels and append to the list\n",
    "    y_pred.extend(np.argmax(preds, axis=-1))\n",
    "\n",
    "# Convert the list of predicted labels to a NumPy array\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Print true and predicted labels\n",
    "print(\"True labels (y_test):\", y_test)\n",
    "print(\"Predicted labels (y_pred):\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dc0fc82-1536-478c-b2d8-83a4a9864a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6666666666666666\n",
      "Precision: 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Calculate recall\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0190507e-512d-4922-b0de-36c9fb3f48eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-29 15:22:02.493958: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [1,9]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "0.013150731\n"
     ]
    }
   ],
   "source": [
    "def create_predictor(model, model_name, max_len):\n",
    "    \"\"\"\n",
    "    Create a predictor function for making predictions using a pre-trained model.\n",
    "\n",
    "    Args:\n",
    "    - model (TFDistilBertForSequenceClassification): Pre-trained model for sequence classification.\n",
    "    - model_name (str): Name of the pre-trained model.\n",
    "    - max_len (int): Maximum length of input sequences.\n",
    "\n",
    "    Returns:\n",
    "    - predict_proba (function): Predictor function that takes input text and returns the probability of the positive class i.e. label with 1.\n",
    "    \"\"\"\n",
    "    # Initialize a DistilBERT tokenizer with the specified model name\n",
    "    tkzr = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    def predict_proba(text):\n",
    "        \"\"\"\n",
    "        Predict the probability of the positive class for the input text.\n",
    "\n",
    "        Args:\n",
    "        - text (str): Input text for prediction.\n",
    "\n",
    "        Returns:\n",
    "        - float: Probability of the positive class.\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        x = [text]\n",
    "        encodings = construct_encodings(x, tkzr, max_len=max_len)\n",
    "        tfdataset = construct_tfdataset(encodings)\n",
    "        tfdataset = tfdataset.batch(1)\n",
    "\n",
    "        # Make predictions using the pre-trained model\n",
    "        logits = model.predict(tfdataset).logits\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1).numpy()\n",
    "        \n",
    "        # Assuming binary classification, return probability of positive class (class 1)\n",
    "        # Adjust this according to your specific classification task\n",
    "        positive_probability = probabilities[:, 1]\n",
    "        return positive_probability[0]\n",
    "\n",
    "    return predict_proba\n",
    "\n",
    "# Create a predictor function using the trained model, tokenizer, and max_len\n",
    "clf = create_predictor(model, MODEL_NAME, MAX_LEN)\n",
    "\n",
    "# Test the predictor function with an example input\n",
    "print(clf('who is the father of biology?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23de5f13-0a98-4a2e-a9ba-1f8e5933ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pre-trained model to the specified directory\n",
    "model.save_pretrained('./model/clf')\n",
    "\n",
    "# Save the model information (model name and maximum sequence length) to a pickle file\n",
    "import pickle\n",
    "with open('./model/info.pkl', 'wb') as f:\n",
    "    pickle.dump((MODEL_NAME, MAX_LEN), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d25b1765-9f83-43a5-b9a1-5e61d167c864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./model/clf were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ./model/clf and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-05-29 15:22:06.411248: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [1,6]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "0.9907828\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model from the specified directory\n",
    "new_model = TFDistilBertForSequenceClassification.from_pretrained('./model/clf')\n",
    "\n",
    "# Load the model information (model name and maximum sequence length) from the pickle file\n",
    "model_name, max_len = pickle.load(open('./model/info.pkl', 'rb'))\n",
    "\n",
    "# Create a predictor function using the loaded model and information\n",
    "clf = create_predictor(new_model, model_name, max_len)\n",
    "\n",
    "# Test the predictor function with an example input\n",
    "print(clf('what are financial markets'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
