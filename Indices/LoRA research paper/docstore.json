{"docstore/data": {"d612cff6-1eaf-4364-bee4-a0215b501b9c": {"__data__": {"id_": "d612cff6-1eaf-4364-bee4-a0215b501b9c", "embedding": null, "metadata": {"page_label": "1", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fdaab2d-a53b-4de3-a946-4e91e7b7fe91", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13"}, "hash": "1b72007e21bf66b4783a125e0a80d6401ef671495bb434f63ea59413dac5e034", "class_name": "RelatedNodeInfo"}}, "text": "Understanding LoRA \u2014 Low Rank\nAdaptation For Finetuning Large\nModels\nMath behind this parameter efficient finetuning method\nBhavin Jawade\u00b7Follow\nPublished inTowards Data Science\u00b74 min read\u00b71 day ago\n154\nFine-tuning large pre-trained models is computationally challenging, often\ninvolving adjustment of millions of parameters. This traditional fine-tuning\napproach, while effective, demands substantial computational resources and\ntime, posing a bottleneck for adapting these models to specific tasks. LoRA\npresented an effective solution to this problem by decomposing the update\nmatrix during finetuing. To study LoRA, let us start by first revisiting\ntraditional finetuing.\nDecomposition of ( \u0394  W )\nIn traditional fine-tuning, we modify a pre-trained neural network\u2019s weights\nto adapt to a new task. This adjustment involves altering the original weight\nmatrix ( W ) of the network. The changes made to ( W ) during fine-tuning", "start_char_idx": 0, "end_char_idx": 930, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "905422ee-0d15-4da5-805d-9a4ffb72dff4": {"__data__": {"id_": "905422ee-0d15-4da5-805d-9a4ffb72dff4", "embedding": null, "metadata": {"page_label": "2", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3bdc2141-c6cd-4078-8a55-25c9d03e4568", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13"}, "hash": "5024db5738c062d99401f0ba5d0b8975ab0ffbf634532912be7156c315ead2f2", "class_name": "RelatedNodeInfo"}}, "text": "are collectively represented by ( \u0394  W ), such that the updated weights can be\nexpressed as ( W + \u0394  W ).\nNow, rather than modifying ( W ) directly, the LoRA approach seeks to\ndecompose ( \u0394  W ). This decomposition is a crucial step in reducing the\ncomputational overhead associated with fine-tuning large models.\nTraditional finetuning can be reimagined us above. Here W is frozen where as \u0394 W is trainable (Image by the\nblog author)\nThe Intrinsic Rank Hypothesis\nOpen in app Sign up Sign in", "start_char_idx": 0, "end_char_idx": 492, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8505fad9-388d-4569-9c85-4c50b22e3b9c": {"__data__": {"id_": "8505fad9-388d-4569-9c85-4c50b22e3b9c", "embedding": null, "metadata": {"page_label": "3", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39a0d793-b3c0-427d-bdc4-ee68c9f44d20", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13"}, "hash": "10cbb9483f016830a55e753adfccfa937e7721d6d03e65d83992d16771bb99b9", "class_name": "RelatedNodeInfo"}}, "text": "The intrinsic rank hypothesis suggests that significant changes to the neural\nnetwork can be captured using a lower-dimensional representation.\nEssentially, it posits that not all elements of ( \u0394  W ) are equally important;\ninstead, a smaller subset of these changes can effectively encapsulate the\nnecessary adjustments.\nIntroducing Matrices ( A ) and ( B )\nBuilding on this hypothesis, LoRA proposes representing ( \u0394  W ) as the\nproduct of two smaller matrices, ( A ) and ( B ), with a lower rank. The\nupdated weight matrix ( W\u2019 ) thus becomes:\n[ W\u2019 = W + BA ]\nIn this equation, ( W ) remains frozen (i.e., it is not updated during training).\nThe matrices ( B ) and ( A ) are of lower dimensionality, with their product (\nBA ) representing a low-rank approximation of ( \u0394  W ).\n154Search Write", "start_char_idx": 0, "end_char_idx": 795, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c972c637-41a3-4146-a283-df081931882f": {"__data__": {"id_": "c972c637-41a3-4146-a283-df081931882f", "embedding": null, "metadata": {"page_label": "4", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0adb7fac-2735-4d7d-80de-72da6248fb62", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13"}, "hash": "19de10fa5e0e4863e42ceff09ab2c4553cb25bf65d9d89d3a1caa104e312f097", "class_name": "RelatedNodeInfo"}}, "text": "\u0394W is decomposed into two matrices A and B where both have lower dimensionality then d x d. (Image by the\nblog author)\nImpact of Lower Rank on Trainable Parameters\nBy choosing matrices ( A ) and ( B ) to have a lower rank ( r ), the number of\ntrainable parameters is significantly reduced. For example, if ( W ) is a ( d x d\n) matrix, traditionally, updating ( W ) would involve ( d \u00b2  ) parameters.\nHowever, with ( B ) and ( A ) of sizes ( d x r ) and ( r x d ) respectively, the total\nnumber of parameters reduces to ( 2dr ), which is much smaller when ( r <<\nd ).\nThe reduction in the number of trainable parameters, as achieved through\nthe Low-Rank Adaptation (LoRA) method, offers several significant benefits,\nparticularly when fine-tuning large-scale neural networks:", "start_char_idx": 0, "end_char_idx": 774, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3edf83f-7df5-4e8f-810d-bfc62d6c4670": {"__data__": {"id_": "b3edf83f-7df5-4e8f-810d-bfc62d6c4670", "embedding": null, "metadata": {"page_label": "5", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cdf46dda-eeb5-46b0-ae84-cb9e4d101fb4", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13"}, "hash": "90a3e7c6b321ab59d2973a643e25a310513e0e8ea4fdd7e55b15ac0b5c1afaff", "class_name": "RelatedNodeInfo"}}, "text": "1. Reduced Memory Footprint: LoRA decreases memory needs by lowering\nthe number of parameters to update, aiding in the management of large-\nscale models.\n2. Faster Training and Adaptation: By simplifying computational demands,\nLoRA accelerates the training and fine-tuning of large models for new\ntasks.\n3. Feasibility for Smaller Hardware: LoRA\u2019s lower parameter count enables\nthe fine-tuning of substantial models on less powerful hardware, like\nmodest GPUs or CPUs.\n4. Scaling to Larger Models: LoRA facilitates the expansion of AI models\nwithout a corresponding increase in computational resources, making\nthe management of growing model sizes more practical.\nIn the context of LoRA, the concept of rank plays a pivotal role in\ndetermining the efficiency and effectiveness of the adaptation process.\nRemarkably, the paper highlights that the rank of the matrices A and B can\nbe astonishingly low, sometimes as low as one.\nAlthough the LoRA paper predominantly showcases experiments within the\nrealm of Natural Language Processing (NLP), the underlying approach of\nlow-rank adaptation holds broad applicability and could be effectively\nemployed in training various types of neural networks across different\ndomains.\nConclusion\nLoRA\u2019s approach to decomposing ( \u0394  W ) into a product of lower rank\nmatrices effectively balances the need to adapt large pre-trained models to\nnew tasks while maintaining computational efficiency. The intrinsic rank", "start_char_idx": 0, "end_char_idx": 1447, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a1475a3-bbab-4c38-adb7-8cd27909976e": {"__data__": {"id_": "9a1475a3-bbab-4c38-adb7-8cd27909976e", "embedding": null, "metadata": {"page_label": "6", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade", "Edward J."]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1df3894a-9d68-4db9-9815-91701a640856", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13"}, "hash": "2a7990ec786523be298829079381c91373fd160cc3aae17220c631472ee152f6", "class_name": "RelatedNodeInfo"}}, "text": "concept is key to this balance, ensuring that the essence of the model\u2019s\nlearning capability is preserved with significantly fewer parameters.\nReferences:\n[1] Hu, Edward J., et al. \u201cLora: Low-rank adaptation of large language\nmodels.\u201d arXiv preprint arXiv:2106.09685 (2021).\nWritten by Bhavin Jawade\n136 Followers\u00b7Writer for Towards Data Science\nCSE PhD Student @ University at BuffaloFollow\nMore from Bhavin Jawade and Towards Data ScienceLarge Language Models Machine Learning Deep Learning Data Science\nTechnology", "start_char_idx": 0, "end_char_idx": 516, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c43046e-4ac9-482b-a76c-703d5353b541": {"__data__": {"id_": "7c43046e-4ac9-482b-a76c-703d5353b541", "embedding": null, "metadata": {"page_label": "7", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8bee31cf-d7b1-4176-8334-cc66d8b178cf", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13"}, "hash": "7528f044c930a80f0e1e3ebb12296b12a325f8e251d1cd600eeeb6983eb8d8a5", "class_name": "RelatedNodeInfo"}}, "text": "Bhavin JawadeinTowards Data Science\nQuantum Computing ?/!\nZeros and ones. This is how we imagined\ncomputing till now. This is what classical\u2026\n4 min read\u00b7Jul 8, 2018\n112 1\nRahul NayakinTowards Data Science\nHow to Convert Any Text Into a\nGraph of Concepts\nA method to convert any text corpus into a\nKnowledge Graph using Mistral 7B.\n12 min read\u00b7Nov 9\n4.99K 43\nPau Blasco i RocainTowards Data Science\nMy Life Stats: I Tracked My Habits\nfor a Year, and This Is What I\u2026\nI measured the time I spent on my daily\nactivities (studying, doing sports, socializing\u2026\n12 min read\u00b7Nov 20\n4.97K 87Bhavin Jawade\nDeep Learning! draft article.\nAs 2020 comes to an end, Deep learning\nturns out to be the most coveted jargon of th\u2026\n2 min read\u00b7Jan 4, 2021\n142 2", "start_char_idx": 0, "end_char_idx": 739, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fec40e72-e78d-4db9-a6d9-05baefff9e1a": {"__data__": {"id_": "fec40e72-e78d-4db9-a6d9-05baefff9e1a", "embedding": null, "metadata": {"page_label": "8", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95891675-6d66-4d2b-a759-575300029c19", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13"}, "hash": "754e39b3f5d53601643903216157d35d6520505310b7b11e046a29d51c6bb985", "class_name": "RelatedNodeInfo"}}, "text": "See all from Bhavin Jawade See all from Towards Data Science\nRecommended from Medium\nanirban sen\nFinetuning LLMs using LoRA\nBefore getting to the meat of the blog i.e.\nFinetuning an LLM, it will be good to have a\u2026\n8 min read\u00b7Sep 29\n97Marko VidrihinGoPenAI\nMixture of Experts (MoE) in AI\nModels Explained\nThe Mixture of Experts (MoE) is offering a\nunique approach to efficiently scaling model\u2026\n5 min read\u00b7Dec 12\n12\nLists\nPredictive Modeling w/\nPython\n20 stories\u00b7715 savesPractical Guides to Machine\nLearning\n10 stories\u00b7819 saves", "start_char_idx": 0, "end_char_idx": 527, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d56a7b1d-a051-4211-bf11-b01fa9b48186": {"__data__": {"id_": "d56a7b1d-a051-4211-bf11-b01fa9b48186", "embedding": null, "metadata": {"page_label": "9", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade", "Andrej Karpathy"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9109b82d-51bc-4ce7-93aa-cc496e946dd5", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13"}, "hash": "924f44d4a64c1befb1e97a31ce16062a84dc6d46308b1d0bb2f4a412c0c0b959", "class_name": "RelatedNodeInfo"}}, "text": "Natural Language Processing\n1024 stories\u00b7498 savesChatGPT prompts\n32 stories\u00b7839 saves\nYoussef HosniinTowards AI\nAndrej Karpathy LLM Paper\nReading List for LLM Mastery\nLLM Paper Recommendation from World\nLeading AI Researcher\n\u00b710 min read\u00b7Dec 7\n649 1A B Vijay Kumar\nFine Tuning LLM: Parameter\nEfficient Fine Tuning (PEFT) \u2014 \u2026\nParameter Efficient Fine Tuning \u2014 LoRA,\nQLoRA \u2014 Concepts\n8 min read\u00b7Aug 9\n281 5\nTales Matos\nParameter-Efficient Fine-Tuning\n(PEFT): a novel approach for fine-\u2026\n4 min read\u00b7Jul 24Benjamin Marie\nDon\u2019t Merge Your LoRA Adapter\nInto a 4-bit LLM\n16-bit and quantization-unaware adapters\n\u00b77 min read\u00b7Nov 20", "start_char_idx": 0, "end_char_idx": 624, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "791749fd-a7d7-4eeb-81be-ae8f66b97bc4": {"__data__": {"id_": "791749fd-a7d7-4eeb-81be-ae8f66b97bc4", "embedding": null, "metadata": {"page_label": "10", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f882a345-0b89-4121-8393-6bef8f37a905", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13"}, "hash": "98180cdd98e1a22f877931c683bf6be123442673fd89640b7175a04360483666", "class_name": "RelatedNodeInfo"}}, "text": "54 87\nSee more recommendations", "start_char_idx": 0, "end_char_idx": 30, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"d612cff6-1eaf-4364-bee4-a0215b501b9c": {"doc_hash": "ffd69cf26d37b0a203952cf4a8c1d18322243052418812eb7d01129c318c4135", "ref_doc_id": "9fdaab2d-a53b-4de3-a946-4e91e7b7fe91"}, "905422ee-0d15-4da5-805d-9a4ffb72dff4": {"doc_hash": "ab6e6194fc12396d2d16bc4509af86265910bba2b96eb16fed631e33b7adb862", "ref_doc_id": "3bdc2141-c6cd-4078-8a55-25c9d03e4568"}, "8505fad9-388d-4569-9c85-4c50b22e3b9c": {"doc_hash": "1badb0f388ae17d9620ba0f7a0494edc146714904cfc09a9eec3ebcd75fbd974", "ref_doc_id": "39a0d793-b3c0-427d-bdc4-ee68c9f44d20"}, "c972c637-41a3-4146-a283-df081931882f": {"doc_hash": "c5a25a548f70003b19b55a1618399857c89ddb106a3ba5cc4a7d93bec696b664", "ref_doc_id": "0adb7fac-2735-4d7d-80de-72da6248fb62"}, "b3edf83f-7df5-4e8f-810d-bfc62d6c4670": {"doc_hash": "ce7d389bc43b9c6d76ae85c6f62e66682662e40daa248d072517f79672df87cb", "ref_doc_id": "cdf46dda-eeb5-46b0-ae84-cb9e4d101fb4"}, "9a1475a3-bbab-4c38-adb7-8cd27909976e": {"doc_hash": "b54a84e56b10270c855ff3ffc203dbb479f90fbfa8dd66a187413e486f7fadb1", "ref_doc_id": "1df3894a-9d68-4db9-9815-91701a640856"}, "7c43046e-4ac9-482b-a76c-703d5353b541": {"doc_hash": "cf56cc37d49ddb7941e121ba43720033ec6dcbc50a9b6c81d4d54d9c06a3ebc2", "ref_doc_id": "8bee31cf-d7b1-4176-8334-cc66d8b178cf"}, "fec40e72-e78d-4db9-a6d9-05baefff9e1a": {"doc_hash": "ccfa61de728b7c494a0cb8542ae673cd84d1e9619451bdf0385e35828928b5ad", "ref_doc_id": "95891675-6d66-4d2b-a759-575300029c19"}, "d56a7b1d-a051-4211-bf11-b01fa9b48186": {"doc_hash": "79afbef2570d48e4c4d3cac8dcc8abfcfb6b1e8427eb454d3f42b27a749c70c7", "ref_doc_id": "9109b82d-51bc-4ce7-93aa-cc496e946dd5"}, "791749fd-a7d7-4eeb-81be-ae8f66b97bc4": {"doc_hash": "65c6ef4f2d67277d8a91a35ff845f8c474f1987793d124f4c6587c1ec117e3df", "ref_doc_id": "f882a345-0b89-4121-8393-6bef8f37a905"}}, "docstore/ref_doc_info": {"9fdaab2d-a53b-4de3-a946-4e91e7b7fe91": {"node_ids": ["d612cff6-1eaf-4364-bee4-a0215b501b9c"], "metadata": {"page_label": "1", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}}, "3bdc2141-c6cd-4078-8a55-25c9d03e4568": {"node_ids": ["905422ee-0d15-4da5-805d-9a4ffb72dff4"], "metadata": {"page_label": "2", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}}, "39a0d793-b3c0-427d-bdc4-ee68c9f44d20": {"node_ids": ["8505fad9-388d-4569-9c85-4c50b22e3b9c"], "metadata": {"page_label": "3", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}}, "0adb7fac-2735-4d7d-80de-72da6248fb62": {"node_ids": ["c972c637-41a3-4146-a283-df081931882f"], "metadata": {"page_label": "4", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}}, "cdf46dda-eeb5-46b0-ae84-cb9e4d101fb4": {"node_ids": ["b3edf83f-7df5-4e8f-810d-bfc62d6c4670"], "metadata": {"page_label": "5", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}}, "1df3894a-9d68-4db9-9815-91701a640856": {"node_ids": ["9a1475a3-bbab-4c38-adb7-8cd27909976e"], "metadata": {"page_label": "6", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade", "Edward J."]}}, "8bee31cf-d7b1-4176-8334-cc66d8b178cf": {"node_ids": ["7c43046e-4ac9-482b-a76c-703d5353b541"], "metadata": {"page_label": "7", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}}, "95891675-6d66-4d2b-a759-575300029c19": {"node_ids": ["fec40e72-e78d-4db9-a6d9-05baefff9e1a"], "metadata": {"page_label": "8", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}}, "9109b82d-51bc-4ce7-93aa-cc496e946dd5": {"node_ids": ["d56a7b1d-a051-4211-bf11-b01fa9b48186"], "metadata": {"page_label": "9", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade", "Andrej Karpathy"]}}, "f882a345-0b89-4121-8393-6bef8f37a905": {"node_ids": ["791749fd-a7d7-4eeb-81be-ae8f66b97bc4"], "metadata": {"page_label": "10", "file_name": "Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_path": "/home/ubuntu/LoRA research paper/Understanding LoRA \u2014 Low Rank Adaptation For Finetuning Large Models _ by Bhavin Jawade _ Dec, 2023 _ Towards Data Science-1.pdf", "file_type": "application/pdf", "file_size": 1798660, "creation_date": "2024-06-13", "last_modified_date": "2024-06-13", "entities": ["Bhavin Jawade"]}}}}