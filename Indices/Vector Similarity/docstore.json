{"docstore/data": {"6eb92892-599c-497c-aae1-114e97802b18": {"__data__": {"id_": "6eb92892-599c-497c-aae1-114e97802b18", "embedding": null, "metadata": {"page_label": "1", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03", "entities": ["USA", "Cornell University New York", "Nathan Kallus", "Los Gatos", "Netflix Inc.", "NY", "Harald Steck", "CA", "Los Angeles"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b3c20ffa-29a6-4812-83bf-93574c3e6e9c", "node_type": "4", "metadata": {"page_label": "1", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "5801bdfb46cccf0dc530ca823546fd2a07eadb4a2871a961eb6a76b4201df0fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "05f4cdec-fa18-43b0-bac7-0e78bd760178", "node_type": "1", "metadata": {}, "hash": "cbca8fb31203501c4b92bc03c647e0a48c82ea4cd3003a54a354640e562396ce", "class_name": "RelatedNodeInfo"}}, "text": "Is Cosine-Similarity of Embeddings Really About\nSimilarity?\nHarald Steck\nhsteck@netflix.com\nNetflix Inc.\nLos Gatos, CA, USAChaitanya Ekanadham\ncekanadham@netflix.com\nNetflix Inc.\nLos Angeles, CA, USA\nNathan Kallus\nnkallus@netflix.com\nNetflix Inc. & Cornell University\nNew York, NY, USA\nMarch 11, 2024\nAbstract\nCosine-similarity is the cosine of the angle between two vectors, or\nequivalently the dot product between their normalizations. A popular\napplication is to quantify semantic similarity between high-dimensional\nobjects by applying cosine-similarity to a learned low-dimensional feature\nembedding. This can work better but sometimes also worse than the un-\nnormalized dot-product between embedded vectors in practice. To gain\ninsight into this empirical observation, we study embeddings derived from\nregularized linear models, where closed-form solutions facilitate analytical\ninsights. We derive analytically how cosine-similarity can yield arbitrary\nand therefore meaningless \u2018similarities.\u2019 For some linear models the simi-\nlarities are not even unique, while for others they are implicitly controlled\nby the regularization. We discuss implications beyond linear models: a\ncombination of different regularizations are employed when learning deep\nmodels; these have implicit and unintended effects when taking cosine-\nsimilarities of the resulting embeddings, rendering results opaque and\npossibly arbitrary. Based on these insights, we caution against blindly\nusing cosine-similarity and outline alternatives.\n1 Introduction\nDiscrete entities are often embedded via a learned mapping to dense real-valued\nvectors in a variety of domains. For instance, words are embedded based on\ntheir surrounding context in a large language model (LLM), while recommender\n1arXiv:2403.05440v1  [cs.IR]  8 Mar 2024", "start_char_idx": 0, "end_char_idx": 1808, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05f4cdec-fa18-43b0-bac7-0e78bd760178": {"__data__": {"id_": "05f4cdec-fa18-43b0-bac7-0e78bd760178", "embedding": null, "metadata": {"page_label": "2", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "84a5ccc9-b500-4b06-8925-a47ccb1f590b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "8eed1c927bc7558f5da7df7d21c6afd5802dc04837b0127c3e1e13829dc0657f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6eb92892-599c-497c-aae1-114e97802b18", "node_type": "1", "metadata": {"page_label": "1", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "5801bdfb46cccf0dc530ca823546fd2a07eadb4a2871a961eb6a76b4201df0fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "479150be-04c4-4587-a053-fe5c3a5818c4", "node_type": "1", "metadata": {}, "hash": "b7877851f55c8e344bc46d329a0104305af4dd4e2f6b9b0734bd5a06f2269520", "class_name": "RelatedNodeInfo"}}, "text": "systems often learn an embedding of items (and users) based on how they are\nconsumed by users. The benefits of such embeddings are manifold. In particular,\nthey can be used directly as (frozen or fine-tuned) inputs to other models, and/or\nthey can provide a data-driven notion of (semantic) similarity between entities\nthat were previously atomic and discrete.\nWhile similarity in \u2019cosine similarity\u2019 refers to the fact that larger values\n(as opposed to smaller values in distance metrics) indicate closer proximity, it\nhas, however, also become a very popular measure of semantic similarity be-\ntween the entities of interest, the motivation being that the norm of the learned\nembedding-vectors is not as important as the directional alignment between the\nembedding-vectors. While there are countless papers that report the successful\nuse of cosine similarity in practical applications, it was, however, also found\nto not work as well as other approaches, like the (unnormalized) dot-product\nbetween the learned embeddings, e.g., see [3, 4, 8].\nIn this paper, we try to shed light on these inconsistent empirical observa-\ntions. We show that cosine similarity of the learned embeddings can in fact\nyield arbitrary results. We find that the underlying reason is not cosine simi-\nlarity itself, but the fact that the learned embeddings have a degree of freedom\nthat can render arbitrary cosine-similarities even though their (unnormalized)\ndot-products are well-defined and unique. To obtain insights that hold more\ngenerally, we derive analytical solutions, which is possible for linear Matrix Fac-\ntorization (MF) models\u2013this is outlined in detail in the next Section. In Section\n3, we propose possible remedies. The experiments in Section 4 illustrate our\nfindings derived in this paper.\n2 Matrix Factorization Models\nIn this paper, we focus on linear models as they allow for closed-form solutions,\nand hence a theoretical understanding of the limitations of the cosine-similarity\nmetric applied to learned embeddings. We are given a matrix X\u2208Rn\u00d7pcon-\ntaining ndata points and pfeatures (e.g., users and items, respectively, in case of\nrecommender systems). The goal in matrix-factorization (MF) models, or equiv-\nalently in linear autoencoders, is to estimate a low-rank matrix AB\u22a4\u2208Rp\u00d7p,\nwhere A, B\u2208Rp\u00d7kwith k\u2264p, such that the product XABTis a good approx-\nimation of X:1X\u2248XAB\u22a4. When the given Xis a user-item matrix, the rows\n\u20d7biofBare typically referred to as the ( k-dimensional) item-embeddings, while\nthe rows of XA, denoted by \u20d7 xu\u00b7A, can be interpreted as the user-embeddings,\nwhere the embedding of user uis the sum of the item-embeddings \u20d7 ajthat the\nuser has consumed.\nNote that this model is defined in terms of the (unnormalized) dot-product\n1Note that we omitted bias-terms (constant offsets) here for clarity of notation\u2013they can\nsimply be introduced in the preprocessing step by subtracting them from each column or row\nofX. Given that such bias terms can reduce the popularity-bias of the learned embeddings to\nsome degree, they can have some impact regarding the learned similarities, but it is ultimately\nlimited.\n2", "start_char_idx": 0, "end_char_idx": 3137, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "479150be-04c4-4587-a053-fe5c3a5818c4": {"__data__": {"id_": "479150be-04c4-4587-a053-fe5c3a5818c4", "embedding": null, "metadata": {"page_label": "3", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1aa5efed-9681-4615-b1fe-0d55292a77a0", "node_type": "4", "metadata": {"page_label": "3", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "ad0648fc34e59529c317360586f4941c589458f96f34e629cb07bf463b323efe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "05f4cdec-fa18-43b0-bac7-0e78bd760178", "node_type": "1", "metadata": {"page_label": "2", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "8eed1c927bc7558f5da7df7d21c6afd5802dc04837b0127c3e1e13829dc0657f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4ef072f5-1696-4b3e-9738-b637660d82e7", "node_type": "1", "metadata": {}, "hash": "92c8ee0d8f9be9edf0590067f263419b1236ca8029551b5f4d64b7947d66af8e", "class_name": "RelatedNodeInfo"}}, "text": "between the user and item embeddings ( XAB\u22a4)u,i=\u27e8\u20d7 xu\u00b7A,\u20d7bi\u27e9. Nevertheless,\nonce the embeddings have been learned, it is common practice to also consider\ntheir cosine-similarity, between two items cosSim (\u20d7bi,\u20d7bi\u2032), two users cosSim (\u20d7 xu\u00b7\nA, \u20d7 xu\u2032\u00b7A), or a user and an item cosSim (\u20d7 xu\u00b7A,\u20d7bi). In the following, we show\nthat this can lead to arbitrary results, and they may not even be unique.\n2.1 Training\nA key factor affecting the utility of cosine-similarity metric is the regularization\nemployed when learning the embeddings in A, B, as outlined in the following.\nConsider the following two, commonly used, regularization schemes (which both\nhave closed-form solutions, see Sections 2.2 and 2.3:\nmin\nA,B||X\u2212XAB\u22a4||2\nF+\u03bb||AB\u22a4||2\nF (1)\nmin\nA,B||X\u2212XAB\u22a4||2\nF+\u03bb(||XA||2\nF+||B||2\nF) (2)\nThe two training objectives obviously differ in their L2-norm regularization:\n\u2022In the first objective, ||AB\u22a4||2\nFapplies to their product. In linear mod-\nels, this kind of L2-norm regularization can be shown to be equivalent\nto learning with denoising, i.e., drop-out in the input layer, e.g., see [6].\nMoreover, the resulting prediction accuracy on held-out test-data was ex-\nperimentally found to be superior to the one of the second objective [2].\nNot only in MF models, but also in deep learning it is often observed that\ndenoising or drop-out (this objective) leads to better results on held-out\ntest-data than weight decay (second objective) does.\n\u2022The second objective is equivalent to the usual matrix factorization ob-\njective min W||X\u2212PQ\u22a4||2\nF+\u03bb(||P||2\nF+||Q||2\nF), where Xis factorized as\nPQ\u22a4, and P=XAandQ=B. This equivalence is outlined, e.g., in\n[2]. Here, the key is that each matrix PandQis regularized separately,\nsimilar to weight decay in deep learning.\nIf\u02c6Aand \u02c6Bare solutions to either objective, it is well known that then also\n\u02c6ARand \u02c6BRwith an arbitrary rotation matrix R\u2208Rk\u00d7k, are solutions as\nwell. While cosine similarity is invariant under such rotations R, one of the\nkey insights in this paper is that the first (but not the second) objective is\nalso invariant to rescalings of the columns of AandB(i.e., the different latent\ndimensions of the embeddings): if \u02c6A\u02c6B\u22a4is a solution of the first objective, so\nis\u02c6ADD\u22121\u02c6B\u22a4where D\u2208Rk\u00d7kis an arbitrary diagonal matrix. We can hence\ndefine a new solution (as a function of D) as follows:\n\u02c6A(D):= \u02c6AD and\n\u02c6B(D):= \u02c6BD\u22121. (3)\n3", "start_char_idx": 0, "end_char_idx": 2383, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ef072f5-1696-4b3e-9738-b637660d82e7": {"__data__": {"id_": "4ef072f5-1696-4b3e-9738-b637660d82e7", "embedding": null, "metadata": {"page_label": "4", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03", "entities": ["Dcannot", "Bare"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "56726b94-4c8c-46c8-ab39-003405f327bf", "node_type": "4", "metadata": {"page_label": "4", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "fcc54ead4deadf111ea4644b2a5cc8dcd06f71f65f498d6e937b109d51181525", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "479150be-04c4-4587-a053-fe5c3a5818c4", "node_type": "1", "metadata": {"page_label": "3", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "ad0648fc34e59529c317360586f4941c589458f96f34e629cb07bf463b323efe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b4f80411-e8f4-49be-8ce6-a1021d6eb7cc", "node_type": "1", "metadata": {}, "hash": "1b0f5caa4c410d2f2326753a2afc004fd790ecc90fd0989023eb0933f2d70108", "class_name": "RelatedNodeInfo"}}, "text": "In turn, this diagonal matrix Daffects the normalization of the learned user\nand item embeddings (i.e., rows):\n(X\u02c6A(D))(normalized) = \u2126 AX\u02c6A(D)= \u2126AX\u02c6AD and\n\u02c6B(D)\n(normalized)= \u2126 B\u02c6B(D)= \u2126B\u02c6BD\u22121, (4)\nwhere \u2126 Aand \u2126 Bare appropriate diagonal matrices to normalize each learned\nembedding (row) to unit Euclidean norm. Note that in general the matrices do\nnot commute, and hence a different choice for Dcannot (exactly) be compen-\nsated by the normalizing matrices \u2126 Aand \u2126 B. As they depend on D, we make\nthis explicit by \u2126 A(D) and \u2126 B(D).Hence, also the cosine similarities of the\nembeddings depend on this arbitrary matrix D.\nAs one may consider the cosine-similarity between two items, two users, or\na user and an item, the three combinations read\n\u2022item \u2013 item:\ncosSim\u0010\n\u02c6B(D),\u02c6B(D)\u0011\n= \u2126B(D)\u00b7\u02c6B\u00b7D\u22122\u00b7\u02c6B\u22a4\u00b7\u2126B(D)\n\u2022user \u2013 user:\ncosSim\u0010\nX\u02c6A(D), X\u02c6A(D)\u0011\n= \u2126A(D)\u00b7X\u02c6A\u00b7D2\u00b7(X\u02c6A)\u22a4\u00b7\u2126A(D)\n\u2022user \u2013 item:\ncosSim\u0010\nX\u02c6A(D),\u02c6B(D)\u0011\n= \u2126A(D)\u00b7X\u02c6A\u00b7\u02c6B\u22a4\u00b7\u2126B(D)\nIt is apparent that the cosine-similarity in all three combinations depends on\nthe arbitrary diagonal matrix D: while they all indirectly depend on Ddue\nto its effect on the normalizing matrices \u2126 A(D) and \u2126 B(D), note that the\n(particularly popular) item-item cosine-similarity (first line) in addition depends\ndirectly on D(and so does the user-user cosine-similarity, see second item).\n2.2 Details on First Objective (Eq. 1)\nThe closed-form solution of the training objective in Eq. 1 was derived in [2]\nand reads \u02c6A(1)\u02c6B\u22a4\n(1)=Vk\u00b7dMat( ...,1\n1+\u03bb/\u03c32\ni, ...)k\u00b7V\u22a4\nk, where X=:U\u03a3V\u22a4is\nthe singular value decomposition (SVD) of the given data matrix X, where\n\u03a3 = dMat( ..., \u03c3 i, ...) denotes the diagonal matrix of singular values, while U, V\ncontain the left and right singular vectors, respectively. Regarding the klargest\neigenvalues \u03c3i, we denote the truncated matrices of rank kasUk, Vkand ( ...)k.\nWe may define2\n\u02c6A(1)=\u02c6B(1):=Vk\u00b7dMat( ...,1\n1 +\u03bb/\u03c32\ni, ...)1\n2\nk. (5)\n2AsDis arbitrary, we chose to assign dMat( ...,1\n1+\u03bb/\u03c32\ni,...)1\n2\nkto each of \u02c6A,\u02c6Bwithout loss\nof generality.\n4", "start_char_idx": 0, "end_char_idx": 2015, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4f80411-e8f4-49be-8ce6-a1021d6eb7cc": {"__data__": {"id_": "b4f80411-e8f4-49be-8ce6-a1021d6eb7cc", "embedding": null, "metadata": {"page_label": "5", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "921ad561-3d77-43ea-8cd2-5608e7206b9f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "444aece4f7c330ab8d64f6714703016b8aa74f68a0c1fcdf3b21eede17783dd7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ef072f5-1696-4b3e-9738-b637660d82e7", "node_type": "1", "metadata": {"page_label": "4", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "fcc54ead4deadf111ea4644b2a5cc8dcd06f71f65f498d6e937b109d51181525", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be4466d5-1106-48af-a617-21d80ebad3bb", "node_type": "1", "metadata": {}, "hash": "360187f9aafe966b2ba594fe0686ef98e8fa0eff67c276be06cd1c7ae7a14610", "class_name": "RelatedNodeInfo"}}, "text": "The arbitrariness of cosine-similarity becomes especially striking here when we\nconsider the special case of a full-rank MF model, i.e., when k=p. This is\nillustrated by the following two cases:\n\u2022if we choose D= dMat( ...,1\n1+\u03bb/\u03c32\ni, ...)1\n2, then we have \u02c6A(D)\n(1)=\u02c6A(1)\u00b7D=\nV\u00b7dMat( ...,1\n1+\u03bb/\u03c32\ni, ...) and \u02c6B(D)\n(1)=\u02c6B(1)\u00b7D\u22121=V. Given that the full-\nrank matrix of singular vectors Vis already normalized (regarding both\ncolumns and rows), the normalization \u2126 B=Ihence equals the identity\nmatrix I. We thus obtain regarding the item-item cosine-similarities:\ncosSim\u0010\n\u02c6B(D)\n(1),\u02c6B(D)\n(1)\u0011\n=V V\u22a4=I,\nwhich is quite a bizarre result, as it says that the cosine-similarity between\nany pair of (different) item-embeddings is zero, i.e., an item is only similar\nto itself, but not to any other item!\nAnother remarkable result is obtained for the user-item cosine-similarity:\ncosSim\u0010\nX\u02c6A(D)\n(1),\u02c6B(D)\n(1)\u0011\n= \u2126 A\u00b7X\u00b7V\u00b7dMat( ...,1\n1 +\u03bb/\u03c32\ni, ...)\u00b7V\u22a4\n= \u2126 A\u00b7X\u00b7\u02c6A(1)\u02c6B\u22a4\n(1),\nas the only difference to the (unnormalized) dot-product is due to the ma-\ntrix \u2126 A, which normalizes the rows\u2014hence, when we consider the ranking\nof the items for a given user based on the predicted scores, cosine-similarity\nand (unnormalized) dot-product result in exactly the same ranking of the\nitems as the row-normalization is only an irrelevant constant in this case.\n\u2022if we choose D= dMat( ...,1\n1+\u03bb/\u03c32\ni, ...)\u22121\n2, then we have analogously to\nthe previous case: \u02c6B(D)\n(1)=V\u00b7dMat( ...,1\n1+\u03bb/\u03c32\ni, ...), and \u02c6A(D)\n(1)=Vis\northonormal. We now obtain regarding the user-user cosine-similarities:\ncosSim\u0010\nX\u02c6A(D)\n(1), X\u02c6A(D)\n(1)\u0011\n= \u2126A\u00b7X\u00b7X\u22a4\u00b7\u2126A,\ni.e., now the user-similarities are simply based on the raw data-matrix X,\ni.e., without any smoothing due to the learned embeddings. Concerning\nthe user-item cosine-similarities, we now obtain\ncosSim\u0010\nX\u02c6A(D)\n(1),\u02c6B(D)\n(1)\u0011\n= \u2126A\u00b7X\u00b7\u02c6A(1)\u00b7\u02c6B\u22a4\n(1)\u00b7\u2126B,\ni.e., now \u2126 Bnormalizes the rows of B, which we did not have in the\nprevious choice of D.\nSimilarly, the item-item cosine-similarities\ncosSim\u0010\n\u02c6B(D)\n(1),\u02c6B(D)\n(1)\u0011\n= \u2126B\u00b7V\u00b7dMat( ...,1\n1 +\u03bb/\u03c32\ni, ...)2\u00b7V\u22a4\u00b7\u2126B\nare very different from the bizarre result we obtained in the previous choice\nofD.\n5", "start_char_idx": 0, "end_char_idx": 2149, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be4466d5-1106-48af-a617-21d80ebad3bb": {"__data__": {"id_": "be4466d5-1106-48af-a617-21d80ebad3bb", "embedding": null, "metadata": {"page_label": "6", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03", "entities": ["Second Objective"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ee7d2b42-de45-4db2-b3fb-dcf3cd1593b3", "node_type": "4", "metadata": {"page_label": "6", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "dceba9bb1a868504fd2c67368a57e165bfb9dbb91b3cbc59a1476a19aaa9310b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b4f80411-e8f4-49be-8ce6-a1021d6eb7cc", "node_type": "1", "metadata": {"page_label": "5", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "444aece4f7c330ab8d64f6714703016b8aa74f68a0c1fcdf3b21eede17783dd7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2ba18ac-53c9-4c18-bc26-b88b8edc8c7b", "node_type": "1", "metadata": {}, "hash": "472503bac460ac48baa93428d65454f26f8ac5d22f693731e061afd382a297e7", "class_name": "RelatedNodeInfo"}}, "text": "Overall, these two cases show that different choices for Dresult in different\ncosine-similarities, even though the learned model \u02c6A(D)\n(1)\u02c6B(D)\u22a4\n(1)=\u02c6A(1)\u02c6B\u22a4\n(1)is\ninvariant to D. In other words, the results of cosine-similarity are arbitray and\nnot unique for this model.\n2.3 Details on Second Objective (Eq. 2)\nThe solution of the training objective in Eq. 2 was derived in [7] and reads\n\u02c6A(2)=Vk\u00b7dMat( ...,r\n1\n\u03c3i\u00b7(1\u2212\u03bb\n\u03c3i)+, ...)k and\n\u02c6B(2)=Vk\u00b7dMat( ...,r\n\u03c3i\u00b7(1\u2212\u03bb\n\u03c3i)+, ...)k (6)\nwhere ( y)+= max(0 , y), and again X=:U\u03a3V\u22a4is the SVD of the train-\ning data X, and \u03a3 = dMat( ..., \u03c3 i, ...). Note that, if we use the usual no-\ntation of MF where P=XA andQ=B, we obtain \u02c6P=X\u02c6A(2)=Uk\u00b7\ndMat( ...,q\n\u03c3i\u00b7(1\u2212\u03bb\n\u03c3i)+, ...)k, where we can see that here the diagonal ma-\ntrix dMat( ...,q\n\u03c3i\u00b7(1\u2212\u03bb\n\u03c3i)+, ...)kis the same for the user-embeddings and the\nitem-embeddings in Eq. 6, as expected due to the symmetry in the L2-norm\nregularization ||P||2\nF+||Q||2\nFin the training objective in Eq. 2.\nThe key difference to the first training objective (see Eq. 1) is that here the\nL2-norm regularization ||P||2\nF+||Q||2\nFis applied to each matrix individually,\nso that this solution is unique (up to irrelevant rotations, as mentioned above),\ni.e., in this case there is no way to introduce an arbitrary diagonal matrix D\ninto the solution of the second objective. Hence, the cosine-similarity applied\nto the learned embeddings of this MF variant yields unique results.\nWhile this solution is unique, it remains an open question if this unique\ndiagonal matrix dMat( ...,q\n\u03c3i\u00b7(1\u2212\u03bb\n\u03c3i)+, ...)kregarding the user and item em-\nbeddings yields the best possible semantic similarities in practice. If we believe,\nhowever, that this regularization makes the cosine-similarity useful concerning\nsemantic similarity, we could compare the forms of the diagonal matrices in both\nvariants, i.e., comparing Eq. 6 with Eq. 5 suggests that the arbitrary diagonal\nmatrix Din the first variant (see section above) analogously may be chosen as\nD= dMat( ...,p\n1/\u03c3i, ...)k.\n3 Remedies and Alternatives to Cosine-Similarity\nAs we showed analytically above, when a model is trained w.r.t. the dot-\nproduct, its effect on cosine-similarity can be opaque and sometimes not even\nunique. One solution obviously is to train the model w.r.t. to cosine similarity,\nwhich layer normalization [1] may facilitate. Another approach is to avoid the\nembedding space, which caused the problems outlined above in the first place,\n6", "start_char_idx": 0, "end_char_idx": 2473, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2ba18ac-53c9-4c18-bc26-b88b8edc8c7b": {"__data__": {"id_": "a2ba18ac-53c9-4c18-bc26-b88b8edc8c7b", "embedding": null, "metadata": {"page_label": "7", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03", "entities": ["Bobtained"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6a33d2a-f633-432e-8361-93bd2115b712", "node_type": "4", "metadata": {"page_label": "7", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "ca22d589945a033d6de4bb4a3e8314b056f91e5d1bf5f3acb88037e591912694", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be4466d5-1106-48af-a617-21d80ebad3bb", "node_type": "1", "metadata": {"page_label": "6", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "dceba9bb1a868504fd2c67368a57e165bfb9dbb91b3cbc59a1476a19aaa9310b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08a69143-3d57-4a45-9238-62fb8da4f70d", "node_type": "1", "metadata": {}, "hash": "192686f990e9f1f6996fff45a9d4cc50b680bbb14f0f81655f99efd801079bf8", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: Illustration of the large variability of item-item cosine similarities\ncosSim (B, B) on the same data due to different modeling choices. Left: ground-\ntruth clusters (items are sorted by cluster assignment, and within each cluster\nby descending baseline popularity). After training w.r.t. Eq. 1, which allows\nfor arbitrary re-scaling of the singular vectors in Vk, the center three plots show\nthree particular choices of re-scaling, as indicated above each plot. Right: based\non (unique) Bobtained when training w.r.t. Eq. 2.\nand project it back into the original space, where the cosine-similarity can then\nbe applied. For instance, using the models above, and given the raw data X,\none may view X\u02c6A\u02c6B\u22a4as its smoothed version, and the rows of X\u02c6A\u02c6B\u22a4as the\nusers\u2019 embeddings in the original space, where cosine-similarity may then be\napplied.\nApart from that, it is also important to note that, in cosine-similarity, nor-\nmalization is applied only after the embeddings have been learned. This can\nnoticeably reduce the resulting (semantic) similarities compared to applying\nsome normalization, or reduction of popularity-bias, before orduring learning.\nThis can be done in several ways. For instance, a default approach in statis-\ntics is to standardize the data X(so that each column has zero mean and unit\nvariance). Common approaches in deep learning include the use of negative\nsampling or inverse propensity scaling (IPS) as to account for the different item\npopularities (and user activity-levels). For instance, in word2vec [5], a matrix\nfactorization model was trained by sampling negatives with a probability pro-\nportional to their frequency (popularity) in the training data taken to the power\nof\u03b2= 3/4, which resulted in impressive word-similarities at that time.\n4 Experiments\nWhile we discussed the full-rank model above, as it was amenable to analytical\ninsights, we now illustrate these findings experimentally for low-rank embed-\ndings. We are not aware of a good metric for semantic similarity, which moti-\nvated us to conduct experiments on simulated data, so that the ground-truth\nsemantic similarites are known. To this end, we simulated data where items\nare grouped into clusters, and users interact with items based on their cluster\npreferences. We then examined to what extent cosine similarities applied to\nlearned embeddings can recover the item cluster structure.\nIn detail, we generated interactions between n= 20 ,000 users and p=\n7", "start_char_idx": 0, "end_char_idx": 2472, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08a69143-3d57-4a45-9238-62fb8da4f70d": {"__data__": {"id_": "08a69143-3d57-4a45-9238-62fb8da4f70d", "embedding": null, "metadata": {"page_label": "8", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e48c1429-3bb1-40de-96ea-9d3dd1a8fd32", "node_type": "4", "metadata": {"page_label": "8", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "fd099a3f9a90f391ec6bca0d5c8fd8a7e7431b8ab6e7396c52f39db7a643e165", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2ba18ac-53c9-4c18-bc26-b88b8edc8c7b", "node_type": "1", "metadata": {"page_label": "7", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "ca22d589945a033d6de4bb4a3e8314b056f91e5d1bf5f3acb88037e591912694", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25c81e20-5742-4601-ae72-4f06b450fcfb", "node_type": "1", "metadata": {}, "hash": "d4c3885db3c5555c08ed68ddc1e79c4d17bc93f4ad266fb019001fae7006adbc", "class_name": "RelatedNodeInfo"}}, "text": "1,000 items that were randomly assigned to C= 5 clusters with probabilities\npcforc= 1, ..., C . Then we sampled the powerlaw-exponent for each cluster\nc,\u03b2c\u223cUnif( \u03b2(item )\nmin , \u03b2(item )\nmax ) where we chose \u03b2(item )\nmin = 0.25 and \u03b2(item )\nmax =\n1.5, and then assigned a baseline popularity to each item iaccording to the\npowerlaw pi= PowerLaw( \u03b2c). Then we generated the items that each user u\nhad interacted with: first, we randomly sampled user-cluster preferences puc,\nand then computed the user-item probabilities: pui=pucipiP\nipucipi. We sampled the\nnumber of items for this user, ku\u223cPowerLaw( \u03b2(user)), where we used \u03b2(user)=\n0.5, and then sampled kuitems (without replacement) using probabilities pui.\nWe then learned the matrices A, B according to Eq. 1 and also Eq. 2 (with\n\u03bb= 10 ,000 and \u03bb= 100, respectively) from the simulated data. We used a\nlow-rank constraint k= 50\u226ap= 1,000 to complement the analytical results\nfor the full-rank case above.\nFig. 1 shows the \u201dtrue\u201d item-item-similarities as defined by the item clusters\non the left hand side, while the remaining four plots show the item-item cosine\nsimilarities obtained for the following four scenarios: after training w.r.t. Eq. 1,\nwhich allows for arbitrary re-scaling of the singular vectors in Vk(as outlined in\nSection 2.2), the center three cosine-similarities are obtained for three choices of\nre-scaling. The last plot in this row is obtained from training w.r.t. Eq. 2, which\nresults in a unique solution for the cosine-similarities. Again, the main purpose\nhere is to illustrate how vastly different the resulting cosine-similarities can be\neven for reasonable choices of re-scaling when training w.r.t. Eq. 1 (note that\nwe did not use any extreme choice for the re-scaling here, like anti-correlated\nwith the singular values, even though this would also be permitted), and also\nfor the unique solution when training w.r.t. Eq. 2.\nConclusions\nIt is common practice to use cosine-similarity between learned user and/or item\nembeddings as a measure of semantic similarity between these entities. We\nstudy cosine similarities in the context of linear matrix factorization models,\nwhich allow for analytical derivations, and show that cosine similarities are\nheavily dependent on the method and regularization technique, and in some\ncases can be rendered even meaningless. Our analytical derivations are comple-\nmented experimentally by qualitatively examining the output of these models\napplied simulated data where we have ground truth item-item similarity. Based\non these insights, we caution against blindly using cosine-similarity, and pro-\nposed a couple of approaches to mitigate this issue. While this short paper is\nlimited to linear models that allow for insights based on analytical derivations,\nwe expect cosine-similarity of the learned embeddings in deep models to be\nplagued by similar problems, if not larger ones, as a combination of various reg-\nularization methods is typically applied there, and different layers in the model\nmay be subject to different regularization\u2014which implicitly determines a par-\nticular scaling (analogous to matrix Dabove) of the different latent dimensions\nin the learned embeddings in each layer of the deep model, and hence its effect\n8", "start_char_idx": 0, "end_char_idx": 3260, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25c81e20-5742-4601-ae72-4f06b450fcfb": {"__data__": {"id_": "25c81e20-5742-4601-ae72-4f06b450fcfb", "embedding": null, "metadata": {"page_label": "9", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03", "entities": ["R. Jin", "S. Min", "W. Yih . Dense", "D. Chen", "L. Wu", "J. Gao", "Y. Zhou", "P. Lewis", "G. E. Hinton", "V. Karpukhin", "B. Oguz", "Z. Liu", "S. Edunov", "ACM Conference on Knowledge Discovery and Data Mining", "L. Chen", "D. Li", "J. R. Kiros", "J. L. Ba"]}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b0332d1-e269-4c0f-a8b4-be34dedafc7d", "node_type": "4", "metadata": {"page_label": "9", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "645ec16eec07157a80b152257f84735f3af9d811ef8e11ceba9e519d2e591a05", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08a69143-3d57-4a45-9238-62fb8da4f70d", "node_type": "1", "metadata": {"page_label": "8", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}, "hash": "fd099a3f9a90f391ec6bca0d5c8fd8a7e7431b8ab6e7396c52f39db7a643e165", "class_name": "RelatedNodeInfo"}}, "text": "on the resulting cosine similarities may become even more opaque there.\nReferences\n[1] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization, 2016.\narXiv:1607.06450.\n[2] R. Jin, D. Li, J. Gao, Z. Liu, L. Chen, and Y. Zhou. Towards a better\nunderstanding of linear models for recommendation. In ACM Conference\non Knowledge Discovery and Data Mining (KDD) , 2021.\n[3] V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and\nW. Yih. Dense passage retrieval for open-domain question answering, 2020.\narXiv:2004.04906v3.\n[4] O. Khattab and M. Zaharia. ColBERT: Efficient and effective passage search\nvia contextualized late interaction over BERT, 2020. arXiv:2004.12832v2.\n[5] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word\nrepresentations in vector space, 2013.\n[6] H. Steck. Autoencoders that don\u2019t overfit towards the identity. In Advances\nin Neural Information Processing Systems (NeurIPS) , 2020.\n[7] S. Zheng, C. Ding, and F. Nie. Regularized singular value decomposition\nand application to recommender system, 2018. arXiv:1804.05090.\n[8] K. Zhou, K. Ethayarajh, D. Card, and D. Jurafsky. Problems with cosine as\na measure of embedding similarity for high frequency words. In 60th Annual\nMeeting of the Association for Computational Linguistics , 2022.\n9", "start_char_idx": 0, "end_char_idx": 1306, "text_template": "[Excerpt from document]\n{metadata_str}\nExcerpt:\n-----\n{content}\n-----\n", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"6eb92892-599c-497c-aae1-114e97802b18": {"doc_hash": "091738436471595dc4794125113e6ad60005f316ba4aaba03b92616e9a012a4a", "ref_doc_id": "b3c20ffa-29a6-4812-83bf-93574c3e6e9c"}, "05f4cdec-fa18-43b0-bac7-0e78bd760178": {"doc_hash": "8eed1c927bc7558f5da7df7d21c6afd5802dc04837b0127c3e1e13829dc0657f", "ref_doc_id": "84a5ccc9-b500-4b06-8925-a47ccb1f590b"}, "479150be-04c4-4587-a053-fe5c3a5818c4": {"doc_hash": "ad0648fc34e59529c317360586f4941c589458f96f34e629cb07bf463b323efe", "ref_doc_id": "1aa5efed-9681-4615-b1fe-0d55292a77a0"}, "4ef072f5-1696-4b3e-9738-b637660d82e7": {"doc_hash": "91bf2d215bb57a2ceb617a668d22c4c6ba8d7245d23d1bc38cde9c59bb5aab9e", "ref_doc_id": "56726b94-4c8c-46c8-ab39-003405f327bf"}, "b4f80411-e8f4-49be-8ce6-a1021d6eb7cc": {"doc_hash": "444aece4f7c330ab8d64f6714703016b8aa74f68a0c1fcdf3b21eede17783dd7", "ref_doc_id": "921ad561-3d77-43ea-8cd2-5608e7206b9f"}, "be4466d5-1106-48af-a617-21d80ebad3bb": {"doc_hash": "a6903ce7b64927d68de254528f1528dfe1de27782ed08fbf04300c9d6dcb77e4", "ref_doc_id": "ee7d2b42-de45-4db2-b3fb-dcf3cd1593b3"}, "a2ba18ac-53c9-4c18-bc26-b88b8edc8c7b": {"doc_hash": "a8ef97e7f39aaa1b3417407078b0027e184ea757ac0e6512615e8304e38599c7", "ref_doc_id": "d6a33d2a-f633-432e-8361-93bd2115b712"}, "08a69143-3d57-4a45-9238-62fb8da4f70d": {"doc_hash": "fd099a3f9a90f391ec6bca0d5c8fd8a7e7431b8ab6e7396c52f39db7a643e165", "ref_doc_id": "e48c1429-3bb1-40de-96ea-9d3dd1a8fd32"}, "25c81e20-5742-4601-ae72-4f06b450fcfb": {"doc_hash": "844d08654869545f464f83815c1dfdf22627f17489bea05bed65e8c6ec2acdb0", "ref_doc_id": "4b0332d1-e269-4c0f-a8b4-be34dedafc7d"}}, "docstore/ref_doc_info": {"b3c20ffa-29a6-4812-83bf-93574c3e6e9c": {"node_ids": ["6eb92892-599c-497c-aae1-114e97802b18"], "metadata": {"page_label": "1", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03", "entities": ["USA", "Cornell University New York", "Nathan Kallus", "Los Gatos", "Netflix Inc.", "NY", "Harald Steck", "CA", "Los Angeles"]}}, "84a5ccc9-b500-4b06-8925-a47ccb1f590b": {"node_ids": ["05f4cdec-fa18-43b0-bac7-0e78bd760178"], "metadata": {"page_label": "2", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}}, "1aa5efed-9681-4615-b1fe-0d55292a77a0": {"node_ids": ["479150be-04c4-4587-a053-fe5c3a5818c4"], "metadata": {"page_label": "3", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}}, "56726b94-4c8c-46c8-ab39-003405f327bf": {"node_ids": ["4ef072f5-1696-4b3e-9738-b637660d82e7"], "metadata": {"page_label": "4", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03", "entities": ["Dcannot", "Bare"]}}, "921ad561-3d77-43ea-8cd2-5608e7206b9f": {"node_ids": ["b4f80411-e8f4-49be-8ce6-a1021d6eb7cc"], "metadata": {"page_label": "5", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}}, "ee7d2b42-de45-4db2-b3fb-dcf3cd1593b3": {"node_ids": ["be4466d5-1106-48af-a617-21d80ebad3bb"], "metadata": {"page_label": "6", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03", "entities": ["Second Objective"]}}, "d6a33d2a-f633-432e-8361-93bd2115b712": {"node_ids": ["a2ba18ac-53c9-4c18-bc26-b88b8edc8c7b"], "metadata": {"page_label": "7", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03", "entities": ["Bobtained"]}}, "e48c1429-3bb1-40de-96ea-9d3dd1a8fd32": {"node_ids": ["08a69143-3d57-4a45-9238-62fb8da4f70d"], "metadata": {"page_label": "8", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03"}}, "4b0332d1-e269-4c0f-a8b4-be34dedafc7d": {"node_ids": ["25c81e20-5742-4601-ae72-4f06b450fcfb"], "metadata": {"page_label": "9", "file_name": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_path": "/home/ubuntu/Vector Similarity/2403.05440.pdf", "file_type": "application/pdf", "file_size": 1676072, "creation_date": "2024-04-03", "last_modified_date": "2024-04-03", "entities": ["R. Jin", "S. Min", "W. Yih . Dense", "D. Chen", "L. Wu", "J. Gao", "Y. Zhou", "P. Lewis", "G. E. Hinton", "V. Karpukhin", "B. Oguz", "Z. Liu", "S. Edunov", "ACM Conference on Knowledge Discovery and Data Mining", "L. Chen", "D. Li", "J. R. Kiros", "J. L. Ba"]}}}}